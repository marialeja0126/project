"""
APLICACI√ìN STREAMLIT PARA DESPLIEGUE DEL MODELO DE PRECIOS DE VIVIENDAS
"""

import joblib
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from pyod.models.knn import KNN 
import seaborn as sns
from sklearn.preprocessing import MinMaxScaler, StandardScaler 
from sklearn.exceptions import NotFittedError 
import streamlit as st

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, LabelEncoder # A√±adido OneHotEncoder, LabelEncoder
from sklearn.compose import ColumnTransformer # MUY IMPORTANTE para preprocesamiento
from sklearn.pipeline import Pipeline # MUY IMPORTANTE para el flujo
from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier
from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier
from xgboost import XGBRegressor, XGBClassifier # Aseg√∫rate de tener xgboost instalado

# M√©tricas
from sklearn.metrics import mean_squared_error, r2_score, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix


# Configurar la p√°gina
st.set_page_config(
    page_title="Analiza tu dataset",
    page_icon="üìä",
    layout="wide"
)

# Lista de p√°ginas en orden
PAGES = ["Inicio", "Limpieza de Datos", "An√°lisis Exploratorio", "Entrenamiento y Predicci√≥n"]
# Nuevas variables de sesi√≥n (o actualizadas)
if 'target_variable' not in st.session_state: 
    st.session_state.target_variable = None
if 'feature_importances_df' not in st.session_state:
    st.session_state.feature_importances_df = None
if 'selected_model_type' not in st.session_state:
    st.session_state.selected_model_type = None
if 'trained_pipeline' not in st.session_state: 
    st.session_state.trained_pipeline = None
if 'model_performance_metrics' not in st.session_state: # M√©tricas del √∫ltimo modelo entrenado
    st.session_state.model_performance_metrics = None
if 'all_trained_model_metrics' not in st.session_state: # NUEVO: Para comparar todos los modelos
    st.session_state.all_trained_model_metrics = {}
if 'features_used_in_model' not in st.session_state: 
    st.session_state.features_used_in_model = None
if 'problem_type' not in st.session_state: 
    st.session_state.problem_type = None
if 'label_encoder_target' not in st.session_state: 
    st.session_state.label_encoder_target = None
if "reset_app" not in st.session_state:
    st.session_state.reset_app = False

# Limpiar estas nuevas variables de sesi√≥n en limpiar_app()
def limpiar_app():
    st.session_state.reset_app = True

if st.session_state.reset_app:
    keys_to_reset = ['data_uploaded', 'uploaded_file_content', 'df', 
                     'limpieza_aplicada', 'df_limpio', 'scaler_fitted', 
                     'fitted_scaler_instance', 'target_variable',
                     'feature_importances_df', 'selected_model_type', # Nuevas
                     'trained_pipeline', 'model_performance_metrics', # Nuevas
                     'all_trained_model_metrics', # NUEVO
                     'features_used_in_model', 'problem_type', 'label_encoder_target'] # Nuevas
    # ... (resto de tu funci√≥n limpiar_app)
    for key in keys_to_reset:
        if key in st.session_state:
            del st.session_state[key]
    
    st.session_state.data_uploaded = False
    st.session_state.data_uploaded = False
    st.session_state.uploaded_file_content = None
    st.session_state.df = None
    st.session_state.page_index = 0 
    st.session_state.limpieza_aplicada = False
    st.session_state.df_limpio = None
    st.session_state.reset_app = False
    st.session_state.scaler_fitted = False
    st.session_state.fitted_scaler_instance = None # Considerar si este scaler global sigue siendo necesario
    st.session_state.target_variable = None
    st.session_state.feature_importances_df = None
    st.session_state.selected_model_type = None
    st.session_state.trained_pipeline = None
    st.session_state.model_performance_metrics = None
    st.session_state.all_trained_model_metrics = {} # NUEVO
    st.session_state.features_used_in_model = None
    st.session_state.problem_type = None
    st.session_state.label_encoder_target = None
    st.rerun()

# Inicializar estado
if 'data_uploaded' not in st.session_state:
    st.session_state.data_uploaded = False
if 'uploaded_file_content' not in st.session_state:
    st.session_state.uploaded_file_content = None
if 'df' not in st.session_state: 
    st.session_state.df = None
if 'page_index' not in st.session_state:
    st.session_state.page_index = 0
if 'reset_app' not in st.session_state:
    st.session_state.reset_app = False
if 'limpieza_aplicada' not in st.session_state:
    st.session_state.limpieza_aplicada = False
if 'df_limpio' not in st.session_state:
    st.session_state.df_limpio = None
if 'scaler_fitted' not in st.session_state: 
    st.session_state.scaler_fitted = False
if 'fitted_scaler_instance' not in st.session_state: 
    st.session_state.fitted_scaler_instance = None
if 'target_variable' not in st.session_state: # NUEVO: para la variable objetivo
    st.session_state.target_variable = None


# Funci√≥n para reiniciar la app
def limpiar_app():
    st.session_state.reset_app = True

# Ejecutar reinicio si se marc√≥
if st.session_state.reset_app:
    keys_to_reset = ['data_uploaded', 'uploaded_file_content', 'df', 
                     'limpieza_aplicada', 'df_limpio', 'scaler_fitted', 
                     'fitted_scaler_instance', 'target_variable'] # A√±adido target_variable
    for key in keys_to_reset:
        if key in st.session_state:
            del st.session_state[key]
    
    st.session_state.data_uploaded = False
    st.session_state.uploaded_file_content = None
    st.session_state.df = None
    st.session_state.page_index = 0 
    st.session_state.limpieza_aplicada = False
    st.session_state.df_limpio = None
    st.session_state.reset_app = False
    st.session_state.scaler_fitted = False
    st.session_state.fitted_scaler_instance = None
    st.session_state.target_variable = None # Reseteado
    st.rerun()

# Subida de archivo si no hay datos
if not st.session_state.data_uploaded:
    st.title("üîé Analiza tu dataset")
    st.markdown("### Por favor, sube un archivo CSV para comenzar.")
    uploaded_file = st.file_uploader("Subir archivo de datos", type=["csv"], key="file_uploader_main")
    st.markdown("""
                
Con esta aplicaci√≥n, podr√°s de forma sencilla:
                
üîç **Explorar tu Informaci√≥n**  
Sube tu archivo y obt√©n un resumen r√°pido, visualiza tus datos y selecciona tu variable objetivo.

‚ú® **Preparar tus Datos**  
Limpia tu informaci√≥n eliminando datos faltantes o valores at√≠picos para an√°lisis m√°s precisos.

üìä **Descubrir Insights**  
Visualiza patrones y correlaciones entre las variables.

üß† **Entrenar tus Propios Modelos**  
Te ayudaremos a identificar las variables m√°s importantes y podr√°s entrenar modelos predictivos (como Random Forest, √Årboles de Decisi√≥n o XGBoost) para tu variable objetivo.

üéØ **Comparar y Predecir**  
Compara cu√°l modelo funciona mejor con tus datos y √∫salo para hacer nuevas predicciones.
""")


    if uploaded_file is not None:
        st.session_state.uploaded_file_content = uploaded_file 
        st.session_state.data_uploaded = True
        try:
            st.session_state.df = pd.read_csv(uploaded_file)
            st.session_state.target_variable = None # Resetear variable objetivo si se sube nuevo archivo
        except Exception as e:
            st.error(f"Error al leer el archivo CSV: {e}")
            st.session_state.data_uploaded = False 
            st.session_state.df = None
            st.stop()
        st.rerun()
    st.stop()

df = st.session_state.df

if df is None: 
    st.error("Error: El DataFrame no est√° cargado. Intenta subir el archivo de nuevo.")
    if st.button("Reintentar Carga de Archivo"):
        limpiar_app() 
    st.stop()

@st.cache_resource
def load_model_from_path(model_path='models/housing_model.pkl'):
    try:
        model = joblib.load(model_path)
        return model
    except FileNotFoundError:
        return None
    except Exception:
        return None

model = load_model_from_path() 

if st.session_state.fitted_scaler_instance is None:
    st.session_state.fitted_scaler_instance = MinMaxScaler()


# Navegaci√≥n por botones Anterior/Siguiente
page_nav_cols = st.columns([1, 8, 1]) 

with page_nav_cols[0]: 
    if st.session_state.page_index > 0:
        if st.button("‚¨ÖÔ∏è Anterior", use_container_width=True):
            st.session_state.page_index -= 1
            st.rerun()

with page_nav_cols[2]: 
    if st.session_state.page_index < len(PAGES) - 1:
        if st.button("Siguiente ‚û°Ô∏è", use_container_width=True):
            st.session_state.page_index += 1
            st.rerun()

if st.button("üßπ Limpiar y Reiniciar Aplicaci√≥n", type="secondary"):
    limpiar_app()

st.markdown("---") 

page = PAGES[st.session_state.page_index] 

# --- CONTENIDO POR P√ÅGINA ---

if page == "Inicio":
    st.title(f"üîé {page}: Visi√≥n General del Dataset") 
    st.markdown("Bienvenido/a. Aqu√≠ obtendr√°s una visi√≥n general de tus datos y podr√°s definir tu variable objetivo.")
    st.markdown("---")
    # 1. Resumen General del Dataset (sin cambios)
    st.header("1. Resumen General del Dataset")
    if df is not None and not df.empty:
        col_summary1, col_summary2 = st.columns(2)
        with col_summary1:
            st.subheader("üî¢ Dimensiones")
            st.write(f"**N√∫mero de Filas:** {df.shape[0]}")
            st.write(f"**N√∫mero de Columnas:** {df.shape[1]}")
            st.subheader("üìú Tipos de Datos por Columna")
            df_types = df.dtypes.astype(str).reset_index() 
            df_types.columns = ['Columna', 'Tipo de Dato']
            st.dataframe(df_types, use_container_width=True, height=300)
        with col_summary2:
            st.subheader("‚ùì Valores Faltantes")
            missing_values = df.isnull().sum().reset_index()
            missing_values.columns = ['Columna', 'Valores Faltantes']
            missing_values_filtered = missing_values[missing_values['Valores Faltantes'] > 0] 
            if not missing_values_filtered.empty:
                st.dataframe(missing_values_filtered, use_container_width=True, height=300)
            else:
                st.success("‚úÖ ¬°Excelente! No se encontraron valores faltantes en tu dataset.")
        with st.expander("üìä Ver Estad√≠sticas Descriptivas (Columnas Num√©ricas)"):
            st.markdown("""

Estas m√©tricas te ofrecen un resumen general del comportamiento de cada variable num√©rica en tu conjunto de datos:

- **count**: N√∫mero de valores no nulos (sin contar vac√≠os).
- **mean**: Promedio de los valores.
- **std**: Desviaci√≥n est√°ndar, indica qu√© tanto var√≠an los datos respecto a la media.
- **min**: Valor m√≠nimo observado.
- **25% (Q1)**: Primer cuartil, el 25% de los datos son menores o iguales a este valor.
- **50% (Q2 / mediana)**: Valor central de los datos, el 50% de los valores est√°n por debajo y el 50% por encima.
- **75% (Q3)**: Tercer cuartil, el 75% de los datos son menores o iguales a este valor.
- **max**: Valor m√°ximo observado.

Estas estad√≠sticas son √∫tiles para entender la distribuci√≥n, identificar posibles outliers y guiar decisiones de limpieza o transformaci√≥n de los datos.
""")

            numeric_cols = df.select_dtypes(include=np.number)
            if not numeric_cols.empty:
                st.dataframe(numeric_cols.describe().T)
            else:
                st.info("No hay columnas num√©ricas para mostrar estad√≠sticas descriptivas.")
        with st.expander("üìù Ver Estad√≠sticas Descriptivas (Columnas Categ√≥ricas/Objeto)"):

            categorical_cols = df.select_dtypes(include=['object', 'category'])
            st.markdown("""
### üßæ Estad√≠sticas Descriptivas (Columnas Categ√≥ricas / Objeto)

Estas m√©tricas permiten entender la distribuci√≥n general de las variables no num√©ricas:

- **count**: N√∫mero de valores no nulos (sin contar vac√≠os).
- **unique**: Cantidad de valores √∫nicos distintos en la columna.
- **top**: Valor m√°s frecuente (modo).
- **freq**: Frecuencia del valor m√°s com√∫n (cu√°ntas veces aparece el top).

Estas estad√≠sticas son √∫tiles para identificar la categor√≠a predominante, verificar la diversidad de respuestas y detectar posibles valores an√≥malos o dominantes en tus variables categ√≥ricas.
""")

            if not categorical_cols.empty:
                st.dataframe(categorical_cols.describe().T)
            else:
                st.info("No hay columnas categ√≥ricas/objeto para mostrar estad√≠sticas descriptivas.")
        st.markdown("---")
    else:
        st.warning("El DataFrame est√° vac√≠o o no se ha cargado correctamente.")

    # 2. Vista Previa del Dataset (sin cambios)
    st.header("2. Vista Previa del Dataset")
    if df is not None and not df.empty:
        # ... (c√≥digo existente slider y dataframe) ...
        num_rows_preview = st.slider(
            "Selecciona el n√∫mero de filas para previsualizar:",
            min_value=1, max_value=min(20, df.shape[0]), 
            value=min(5, df.shape[0]), key="preview_slider_inicio" 
        )
        st.dataframe(df.head(num_rows_preview), use_container_width=True)
    st.markdown("---")

    # 3. Definir Variable Objetivo (NUEVO)
    st.header("3. Definir Variable Objetivo")
    st.markdown("""
    Seleccionar una variable objetivo ayudar√° a enfocar algunos de los an√°lisis y visualizaciones 
    en las p√°ginas siguientes, especialmente en la secci√≥n de 'An√°lisis Exploratorio'.
    """)

    if df is not None and not df.empty:
        column_options = [None] + df.columns.tolist()
        
        # Determinar el √≠ndice de la selecci√≥n actual o None
        current_target_val = st.session_state.get('target_variable', None)
        if current_target_val in column_options:
            current_target_index = column_options.index(current_target_val)
        else: # Si el target guardado no est√° en las opciones (p.ej. se borr√≥), default a None
            st.session_state.target_variable = None
            current_target_index = 0 # √çndice de None

        selected_target = st.selectbox(
            "Selecciona tu variable objetivo:",
            options=column_options,
            index=current_target_index,
            format_func=lambda x: "No seleccionar" if x is None else x,
            key="target_variable_selector_inicio"
        )

        # Actualizar solo si hay un cambio real para evitar reruns innecesarios si el valor es el mismo
        if selected_target != st.session_state.target_variable:
            st.session_state.target_variable = selected_target
            st.rerun() # Rerun para que el estado se actualice y se refleje en la UI

        if st.session_state.target_variable:
            st.success(f"Variable objetivo seleccionada: **{st.session_state.target_variable}**")
            if st.session_state.target_variable not in df.columns: # Doble chequeo por si acaso
                 st.warning(f"Advertencia: La variable objetivo '{st.session_state.target_variable}' previamente seleccionada ya no existe en el dataset. Por favor, selecciona una nueva.")
                 st.session_state.target_variable = None
                 st.rerun()
        else:
            st.info("No se ha seleccionado una variable objetivo. Algunos an√°lisis espec√≠ficos del objetivo estar√°n desactivados o ser√°n m√°s generales.")



elif page == "Limpieza de Datos":
    st.title(f"üßπ {page}: Preparaci√≥n de Datos") 
    # ... (resto del c√≥digo de Limpieza de Datos sin cambios relevantes a target_variable aqu√≠) ...
    # Importante: Si la limpieza elimina la variable objetivo, el usuario ser√° notificado en la p√°gina de EDA.
    st.markdown("""
    En esta secci√≥n puedes aplicar una limpieza b√°sica al conjunto de datos. 
    Los cambios aqu√≠ afectar√°n las p√°ginas subsiguientes de an√°lisis y predicci√≥n.
    Los datos procesados se guardar√°n como 'datos limpios'.
    """)

    if st.session_state.limpieza_aplicada and st.session_state.df_limpio is not None:
        st.success("üéâ ¬°Los datos ya fueron procesados con los par√°metros de limpieza anteriores!")
        st.markdown("Si deseas aplicar una nueva limpieza, los datos originales se usar√°n como base.")
        if st.button("Restaurar datos originales para nueva limpieza", key="restore_limpieza"):
            st.session_state.limpieza_aplicada = False
            st.rerun()

    df_para_limpiar = df.copy() 
    
    st.subheader("Opciones de Limpieza")
    with st.form("form_limpieza"):
        eliminar_nulos = st.checkbox("Eliminar filas con valores nulos (NaN)", value=True)
        columnas_numericas_limpieza = df_para_limpiar.select_dtypes(include=np.number).columns.tolist()
        if not columnas_numericas_limpieza:
            porcentaje_outliers = 0.0
            st.info("No hay columnas num√©ricas en el dataset para la detecci√≥n de outliers con KNN.")
        else:
            porcentaje_outliers = st.slider("Porcentaje de outliers a identificar y eliminar (usando KNN)", 0.0, 0.25, 0.05, step=0.01,
                                            help="Esto se aplica a las columnas num√©ricas...",
                                            disabled=not bool(columnas_numericas_limpieza))
        aplicar_limpieza = st.form_submit_button("üöÄ Aplicar Limpieza")

    if aplicar_limpieza:
        with st.spinner("Aplicando limpieza... Por favor espera."):
            df_resultado_limpieza = df_para_limpiar.copy()
            original_shape = df_resultado_limpieza.shape
            if eliminar_nulos:
                # ... (l√≥gica de eliminar nulos) ...
                nulos_antes = df_resultado_limpieza.isnull().sum().sum()
                df_resultado_limpieza.dropna(inplace=True)
                nulos_despues = df_resultado_limpieza.isnull().sum().sum()
                st.write(f"Se eliminaron {nulos_antes - nulos_despues} celdas con valores nulos.")
                if df_resultado_limpieza.empty:
                    st.error("El dataset qued√≥ vac√≠o despu√©s de eliminar nulos.")
                    st.stop()

            if porcentaje_outliers > 0 and not df_resultado_limpieza.empty and columnas_numericas_limpieza:
                # ... (l√≥gica de outliers) ...
                columnas_numericas_actuales = df_resultado_limpieza.select_dtypes(include=np.number).columns
                if not columnas_numericas_actuales.empty:
                    df_temp_scaled = df_resultado_limpieza.copy()
                    try:
                        temp_scaler_for_outliers = MinMaxScaler().fit(df_para_limpiar[columnas_numericas_actuales])
                        df_temp_scaled[columnas_numericas_actuales] = temp_scaler_for_outliers.transform(df_temp_scaled[columnas_numericas_actuales])
                        modelo_outliers = KNN(contamination=porcentaje_outliers)
                        modelo_outliers.fit(df_temp_scaled[columnas_numericas_actuales])
                        etiquetas_outliers = modelo_outliers.labels_
                        outliers_eliminados = np.sum(etiquetas_outliers == 1)
                        df_resultado_limpieza = df_temp_scaled[etiquetas_outliers == 0]
                        st.write(f"Se identificaron y eliminaron {outliers_eliminados} filas como outliers.")
                        if df_resultado_limpieza.empty:
                            st.error("El dataset qued√≥ vac√≠o despu√©s de eliminar outliers.")
                            st.stop()
                    except Exception as e:
                        st.error(f"Error durante la detecci√≥n de outliers: {e}.")
                else:
                    st.info("No quedan columnas num√©ricas para la detecci√≥n de outliers despu√©s del primer paso.")
            
            st.session_state.df_limpio = df_resultado_limpieza.reset_index(drop=True)
            st.session_state.limpieza_aplicada = True
            final_shape = st.session_state.df_limpio.shape
            st.write(f"Dimensiones originales: {original_shape}, Dimensiones despu√©s de limpieza: {final_shape}")
        st.success("‚úÖ Limpieza procesada.")
        st.rerun() 

    if st.session_state.limpieza_aplicada and st.session_state.df_limpio is not None:
        # ... (comparaci√≥n de datos) ...
        st.subheader("Comparaci√≥n de Datos")
        col_orig, col_limp = st.columns(2)
        with col_orig:
            st.markdown("**Datos Originales (Primeras filas)**")
            st.write(f"Dimensiones: {df.shape}")
            st.dataframe(df.head(), height=200, use_container_width=True)
        with col_limp:
            st.markdown("**Datos Limpios (Primeras filas)**")
            st.write(f"Dimensiones: {st.session_state.df_limpio.shape}")
            st.dataframe(st.session_state.df_limpio.head(), height=200, use_container_width=True)
        if st.session_state.df_limpio.equals(df) and (df.shape == st.session_state.df_limpio.shape):
             st.info("Los datos limpios son id√©nticos a los originales.")


elif page == "An√°lisis Exploratorio":
    st.title(f"üìä {page}: Entendiendo tus Datos") 
    
    df_eda = st.session_state.df_limpio if st.session_state.limpieza_aplicada and st.session_state.df_limpio is not None else df
    target_var_name = st.session_state.get('target_variable', None)

    if st.session_state.limpieza_aplicada and st.session_state.df_limpio is not None:
        st.info("Mostrando an√°lisis sobre los **datos limpios**.")
    else:
        st.info("Mostrando an√°lisis sobre los **datos originales**. Puedes aplicar la limpieza en la p√°gina 'Limpieza de Datos'.")

    if df_eda.empty:
        st.warning("El dataset para an√°lisis est√° vac√≠o. Verifica los pasos anteriores.")
        st.stop()

    # Verificar si la variable objetivo seleccionada a√∫n existe
    if target_var_name and target_var_name not in df_eda.columns:
        st.warning(f"La variable objetivo '{target_var_name}' fue seleccionada pero ya no existe en el dataset procesado. "
                   "Por favor, selecciona una nueva variable objetivo en la p√°gina de 'Inicio'.")
        st.session_state.target_variable = None # Resetearla
        target_var_name = None # Asegurar que es None para la l√≥gica subsiguiente

    st.markdown("Visualizaciones para entender relaciones entre variables.")
    st.markdown("---")
    
    # Secci√≥n de An√°lisis Enfocado en la Variable Objetivo
    if target_var_name:
        st.header(f"üîç An√°lisis Enfocado en: '{target_var_name}'")
        target_series = df_eda[target_var_name]

        # A. Distribuci√≥n de la Variable Objetivo
        st.subheader(f"A. Distribuci√≥n de '{target_var_name}'")
        fig_target_dist, ax_target_dist = plt.subplots(figsize=(8, 5))
        if pd.api.types.is_numeric_dtype(target_series):
            sns.histplot(target_series, kde=True, ax=ax_target_dist, color="skyblue")
            ax_target_dist.set_title(f"Distribuci√≥n de {target_var_name} (Num√©rica)")
        elif pd.api.types.is_categorical_dtype(target_series) or target_series.dtype == 'object':
            # Limitar el n√∫mero de categor√≠as para el countplot para evitar gr√°ficos muy grandes
            top_n = 20 
            if target_series.nunique() > top_n:
                st.caption(f"Mostrando las {top_n} categor√≠as m√°s frecuentes de '{target_var_name}'.")
                value_counts = target_series.value_counts().nlargest(top_n)
                sns.barplot(x=value_counts.index, y=value_counts.values, ax=ax_target_dist, palette="viridis")
            else:
                 sns.countplot(y=target_series, ax=ax_target_dist, order=target_series.value_counts().index, palette="viridis")
            ax_target_dist.set_title(f"Distribuci√≥n de {target_var_name} (Categ√≥rica)")
            plt.xticks(rotation=45, ha='right')
        else:
            st.info(f"El tipo de dato de '{target_var_name}' ({target_series.dtype}) no es directamente graficable como num√©rica o categ√≥rica aqu√≠.")
        plt.tight_layout()
        st.pyplot(fig_target_dist)
        plt.clf() # Limpiar figura actual

        # B. Relaciones con otras variables (dependiendo del tipo de target)
        st.subheader(f"B. Relaciones de otras variables con '{target_var_name}'")
        other_vars = df_eda.columns.drop(target_var_name, errors='ignore')

        if pd.api.types.is_numeric_dtype(target_series): # Target Num√©rico
            numeric_cols_for_scatter = df_eda[other_vars].select_dtypes(include=np.number).columns
            if len(numeric_cols_for_scatter) > 0:
                default_scatter_cols = numeric_cols_for_scatter.tolist()[:min(4, len(numeric_cols_for_scatter))]
                selected_cols_scatter = st.multiselect(
                    f"Selecciona variables num√©ricas para comparar con '{target_var_name}' (Scatter Plots):",
                    options=numeric_cols_for_scatter.tolist(),
                    default=default_scatter_cols,
                    key="scatter_vs_numeric_target"
                )
                if selected_cols_scatter:
                    cols_per_row = st.number_input("Gr√°ficos por fila (scatter):", 1, 4, 2, key="cols_scatter_target")
                    num_rows = (len(selected_cols_scatter) + cols_per_row - 1) // cols_per_row
                    fig_scatter, axes_scatter = plt.subplots(num_rows, cols_per_row, figsize=(6 * cols_per_row, 5 * num_rows), squeeze=False)
                    axes_scatter = axes_scatter.flatten()
                    for i, col_name in enumerate(selected_cols_scatter):
                        sns.scatterplot(data=df_eda, x=col_name, y=target_var_name, ax=axes_scatter[i], alpha=0.7, color="coral")
                        axes_scatter[i].set_title(f'{col_name} vs {target_var_name}')
                    for j in range(len(selected_cols_scatter), len(axes_scatter)): fig_scatter.delaxes(axes_scatter[j])
                    plt.tight_layout()
                    st.pyplot(fig_scatter)
                    plt.clf()
            else:
                st.info(f"No hay otras variables num√©ricas para generar scatter plots contra '{target_var_name}'.")

        elif pd.api.types.is_categorical_dtype(target_series) or target_series.dtype == 'object': # Target Categ√≥rico
            numeric_cols_for_boxplot = df_eda[other_vars].select_dtypes(include=np.number).columns
            if len(numeric_cols_for_boxplot) > 0:
                selected_numeric_for_boxplot = st.selectbox(
                    f"Selecciona una variable num√©rica para comparar con '{target_var_name}' (Box Plots):",
                    options=numeric_cols_for_boxplot.tolist(),
                    key="boxplot_vs_categorical_target"
                )
                if selected_numeric_for_boxplot:
                    fig_boxplot, ax_boxplot = plt.subplots(figsize=(10, 6))
                    # Limitar categor√≠as en eje X del boxplot si son muchas
                    order_boxplot = target_series.value_counts().nlargest(10).index if target_series.nunique() > 10 else None
                    if order_boxplot is not None: st.caption(f"Mostrando boxplots para las 10 categor√≠as m√°s frecuentes de '{target_var_name}'.")

                    sns.boxplot(data=df_eda, x=target_var_name, y=selected_numeric_for_boxplot, ax=ax_boxplot, palette="Set2", order=order_boxplot)
                    ax_boxplot.set_title(f'{selected_numeric_for_boxplot} por categor√≠as de {target_var_name}')
                    plt.xticks(rotation=45, ha='right')
                    plt.tight_layout()
                    st.pyplot(fig_boxplot)
                    plt.clf()
            else:
                st.info(f"No hay variables num√©ricas para generar box plots contra '{target_var_name}'.")
            # Podr√≠as a√±adir aqu√≠ comparaciones con otras variables categ√≥ricas (e.g., stacked bar charts)
        st.markdown("---")
    else:
        st.info("No se ha seleccionado una variable objetivo en la p√°gina de 'Inicio'. "
                  "Los siguientes an√°lisis son generales.")
        st.markdown("---")

    # An√°lisis Generales (no dependen directamente de una variable objetivo predefinida)
    st.header("üî¨ An√°lisis Generales del Dataset")

    st.subheader("Matriz de Correlaci√≥n (Columnas Num√©ricas)")
    st.markdown("""
### üîó ¬øC√≥mo Interpretar una Matriz de Correlaci√≥n?

Una matriz de correlaci√≥n muestra qu√© tan relacionadas est√°n dos variables num√©ricas entre s√≠. Los valores van de **-1 a 1**:

- **1**: Correlaci√≥n positiva perfecta ‚Äì cuando una variable sube, la otra tambi√©n.
- **0**: Sin correlaci√≥n ‚Äì no hay una relaci√≥n lineal aparente.
- **-1**: Correlaci√≥n negativa perfecta ‚Äì cuando una variable sube, la otra baja.

#### Consejos para interpretar:
- Busca valores cercanos a **1 o -1** para identificar relaciones fuertes.
- Una correlaci√≥n alta no siempre significa causalidad.
- Puedes usar esto para identificar variables redundantes o relevantes para modelos predictivos.

Puedes hacer clic en los valores o usar un heatmap para ver r√°pidamente qu√© pares de variables tienen relaciones fuertes o d√©biles.
""")

    numeric_cols_eda = df_eda.select_dtypes(include=np.number)
    if len(numeric_cols_eda.columns) > 1:
        corr = numeric_cols_eda.corr()
        fig_corr, ax_corr = plt.subplots(figsize=(12, 10))
        mask = np.triu(np.ones_like(corr, dtype=bool))
        cmap = sns.diverging_palette(230, 20, as_cmap=True)
        sns.heatmap(corr, mask=mask, cmap=cmap, vmax=1, vmin=-1, center=0, 
                    square=True, linewidths=.5, annot=True, fmt='.2f', annot_kws={"size": 8}, ax=ax_corr)
        plt.xticks(rotation=45, ha='right'); plt.yticks(rotation=0)
        plt.tight_layout(); st.pyplot(fig_corr); plt.clf()
    else:
        st.info("No hay suficientes columnas num√©ricas para generar una matriz de correlaci√≥n.")

    st.subheader("Distribuci√≥n de Variables Individuales")
    # ... (c√≥digo existente de distribuci√≥n de variables, sin cambios importantes) ...
    dist_cols = df_eda.columns.tolist()
    # Intenta preseleccionar la variable objetivo si est√° definida y existe
    default_dist_index = 0
    if target_var_name and target_var_name in dist_cols:
        default_dist_index = dist_cols.index(target_var_name)
    
    selected_col_dist = st.selectbox(
        "Selecciona una variable para ver su distribuci√≥n:",
        options=dist_cols, index=default_dist_index, key="dist_selector_general"
    )
    if selected_col_dist:
        fig_dist_gen, ax_dist_gen = plt.subplots(figsize=(8,5))
        if pd.api.types.is_numeric_dtype(df_eda[selected_col_dist]):
            sns.histplot(df_eda[selected_col_dist], kde=True, ax=ax_dist_gen, color="teal")
        else: 
            top_n_dist = 20
            if df_eda[selected_col_dist].nunique() > top_n_dist:
                st.caption(f"Mostrando las {top_n_dist} categor√≠as m√°s frecuentes.")
                value_counts_dist = df_eda[selected_col_dist].value_counts().nlargest(top_n_dist)
                sns.barplot(x=value_counts_dist.index, y=value_counts_dist.values, ax=ax_dist_gen, palette="coolwarm")
            else:
                sns.countplot(y=df_eda[selected_col_dist], ax=ax_dist_gen, order = df_eda[selected_col_dist].value_counts().index, palette="coolwarm")
            plt.xticks(rotation=45, ha='right')
        ax_dist_gen.set_title(f'Distribuci√≥n de {selected_col_dist}')
        plt.tight_layout(); st.pyplot(fig_dist_gen); plt.clf()

    st.subheader("Exploraci√≥n Interactiva")
    if len(df_eda.columns) > 1:
        col1_exp, col2_exp = st.columns(2)
        all_cols_eda = df_eda.columns.tolist()
        with col1_exp:
            x_var = st.selectbox("Variable X:", options=all_cols_eda, index=0, key="x_var_eda_general")
        with col2_exp:
            y_var_options = [col for col in all_cols_eda if col != x_var]
            if not y_var_options: y_var_options = all_cols_eda 
            y_var_default_index = 0
            if target_var_name and target_var_name in y_var_options and x_var != target_var_name:
                y_var_default_index = y_var_options.index(target_var_name)
            y_var = st.selectbox("Variable Y:", options=y_var_options, index=y_var_default_index, key="y_var_eda_general")
        if x_var and y_var:
            fig_sc_int, ax_sc_int = plt.subplots(figsize=(8,6))

            hue_var_options = [None] + [col for col in all_cols_eda if col != x_var and col != y_var and df_eda[col].nunique() < 20]
            hue_var = None
            if hue_var_options != [None]:
                hue_var = st.selectbox("Variable para color (Hue, opcional):", options=hue_var_options, index=0, key="hue_var_eda_general")

            try:
                sns.scatterplot(data=df_eda, x=x_var, y=y_var, hue=hue_var, ax=ax_sc_int, palette="magma", alpha=0.7)
                ax_sc_int.set_title(f'{x_var} vs {y_var}' + (f' (Color por {hue_var})' if hue_var else ''))
                if hue_var: ax_sc_int.legend(title=hue_var, bbox_to_anchor=(1.05, 1), loc='upper left')
                plt.tight_layout(); st.pyplot(fig_sc_int); plt.clf()
            except Exception as e:
                st.error(f"No se pudo generar el gr√°fico: {e}")

# ... (c√≥digo de las p√°ginas anteriores) ...

elif page == "Entrenamiento y Predicci√≥n":
    st.title(f"üöÄ {page}: Construye, Eval√∫a y Usa tu Modelo")

    # --- PASO 0: PREPARACI√ìN DE DATOS ---
    if 'df' not in st.session_state or st.session_state.df is None:
        st.warning("Por favor, primero carga un dataset en la p√°gina de 'Inicio'.")
        st.stop()

    df_source = st.session_state.df_limpio if st.session_state.limpieza_aplicada and st.session_state.df_limpio is not None else st.session_state.df
    
    if df_source.empty:
        st.warning("El dataset est√° vac√≠o. Verifica los pasos anteriores.")
        st.stop()

    target_var_name = st.session_state.get('target_variable', None)
    if not target_var_name or target_var_name not in df_source.columns:
        st.error("Variable objetivo no definida o no encontrada. Por favor, selecci√≥nala en la p√°gina de 'Inicio'.")
        st.stop()

    st.info(f"Usando variable objetivo: **{target_var_name}**")
    y = df_source[target_var_name].copy() # Series para la variable objetivo
    X_original = df_source.drop(columns=[target_var_name]).copy() # DataFrame de caracter√≠sticas originales

    # Determinar tipo de problema y preprocesar 'y' para clasificaci√≥n si es necesario
    if pd.api.types.is_numeric_dtype(y):
        st.session_state.problem_type = "Regression"
    elif y.nunique() > 1:
        st.session_state.problem_type = "Classification"
        le_target = LabelEncoder()
        y = pd.Series(le_target.fit_transform(y), name=target_var_name) # y ahora es num√©rico
        st.session_state.label_encoder_target = le_target # Guardar para decodificar predicciones
        st.caption(f"Variable objetivo '{target_var_name}' codificada para clasificaci√≥n. Clases originales: {list(le_target.classes_)}")
    else:
        st.error("La variable objetivo no es adecuada (p.ej., tiene un solo valor √∫nico).")
        st.stop()
    st.write(f"Tipo de problema detectado: **{st.session_state.problem_type}**")
    st.markdown("---")

    # --- PASO 1: C√ÅLCULO Y SELECCI√ìN DE CARACTER√çSTICAS ---
    st.header("1. Selecci√≥n de Caracter√≠sticas")
    
    # Usar solo columnas num√©ricas o categ√≥ricas simples para el c√°lculo de importancia inicial
    # Para RandomForest, es mejor tener todo num√©rico.
    X_for_importance = X_original.copy()
    categorical_cols_for_imp = X_for_importance.select_dtypes(include=['object', 'category']).columns
    
    # Simplificaci√≥n: Para el c√°lculo de importancia, se usar√°n solo las num√©ricas o se intentar√° una codificaci√≥n simple.
    # Una opci√≥n es usar RandomForest que puede dar importancia incluso con OHE, pero el OHE puede diluir la importancia.
    # Otra es usar SelectKBest con f_regression/chi2.
    # Por simplicidad, usaremos RF en las num√©ricas y mostraremos una advertencia para las categ√≥ricas en este paso.
    
    numeric_cols_for_imp = X_for_importance.select_dtypes(include=np.number).columns
    if not numeric_cols_for_imp.empty:
        X_numeric_for_imp = X_for_importance[numeric_cols_for_imp]
        
        if st.button("Calcular Importancia de Caracter√≠sticas Num√©ricas (con Random Forest)", key="calc_imp_btn"):
            with st.spinner("Calculando..."):
                if st.session_state.problem_type == "Regression":
                    model_imp = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)
                else: # Classification
                    model_imp = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)
                
                try:
                    model_imp.fit(X_numeric_for_imp, y) # y ya est√° codificada si es clasificaci√≥n
                    importances = model_imp.feature_importances_
                    st.session_state.feature_importances_df = pd.DataFrame({
                        'Caracter√≠stica': X_numeric_for_imp.columns,
                        'Importancia': importances
                    }).sort_values(by='Importancia', ascending=False).reset_index(drop=True)
                except Exception as e:
                    st.error(f"Error al calcular importancia: {e}")
                    st.session_state.feature_importances_df = None
        
        if st.session_state.feature_importances_df is not None:
            st.subheader("Importancia de Caracter√≠sticas Num√©ricas:")
            num_to_show = st.slider("Mostrar N caracter√≠sticas m√°s importantes:", 
                                    min_value=1, 
                                    max_value=len(st.session_state.feature_importances_df), 
                                    value=min(10, len(st.session_state.feature_importances_df)),
                                    key="num_imp_feat_slider")
            
            fig_imp, ax_imp = plt.subplots(figsize=(10, num_to_show * 0.5 if num_to_show * 0.5 > 4 else 4)) # Ajustar tama√±o
            top_features_df = st.session_state.feature_importances_df.head(num_to_show)
            sns.barplot(x='Importancia', y='Caracter√≠stica', data=top_features_df, ax=ax_imp, palette="viridis")
            ax_imp.set_title(f"Top {num_to_show} Caracter√≠sticas Num√©ricas M√°s Importantes")
            plt.tight_layout()
            st.pyplot(fig_imp)
            plt.clf()

            default_selected_features = st.session_state.feature_importances_df['Caracter√≠stica'].head(min(5, len(st.session_state.feature_importances_df))).tolist()
            st.info(f"Sugerencia (Top {len(default_selected_features)} num√©ricas): {', '.join(default_selected_features)}")
        else:
            default_selected_features = X_original.columns.tolist()[:3] # Fallback
    else:
        st.warning("No hay caracter√≠sticas num√©ricas para calcular la importancia con el m√©todo actual. "
                   "Se usar√°n todas las caracter√≠sticas disponibles para la selecci√≥n.")
        default_selected_features = X_original.columns.tolist()[:3] # Fallback

    if not categorical_cols_for_imp.empty:
        st.caption(f"Caracter√≠sticas categ√≥ricas encontradas: {', '.join(categorical_cols_for_imp)}. "
                   "Estas se codificar√°n (One-Hot Encoding) si las seleccionas para el modelo. "
                   "Su importancia no se calcul√≥ directamente en el gr√°fico anterior.")

    # Selecci√≥n de caracter√≠sticas por el usuario (de las originales)
    st.session_state.features_used_in_model = st.multiselect(
        "Selecciona las caracter√≠sticas para entrenar el modelo (num√©ricas y categ√≥ricas):",
        options=X_original.columns.tolist(),
        default=default_selected_features,
        key="feature_selector_model"
    )

    if not st.session_state.features_used_in_model:
        st.warning("Por favor, selecciona al menos una caracter√≠stica para entrenar.")
        st.stop()
    st.markdown("---")

    # --- PASO 2: SELECCI√ìN DEL TIPO DE MODELO ---
    st.header("2. Selecci√≥n y Configuraci√≥n del Modelo")
    model_choices = ["Random Forest", "Decision Tree", "XGBoost"]
    st.session_state.selected_model_type = st.selectbox("Elige el tipo de modelo:", model_choices, key="model_type_selector")

    # (Opcional) Hiperpar√°metros b√°sicos para cada modelo
    # Ejemplo para Random Forest:
    # if st.session_state.selected_model_type == "Random Forest":
    #     n_estimators = st.slider("N√∫mero de √°rboles (n_estimators):", 50, 500, 100, step=10)
    #     max_depth = st.slider("Profundidad m√°xima (max_depth):", 3, 20, 10, step=1)
    st.markdown("---")

    # --- PASO 3: ENTRENAMIENTO DEL MODELO ---
    st.header("3. Entrenamiento y Evaluaci√≥n del Modelo")
    if st.button(f"üöÄ Entrenar Modelo: {st.session_state.selected_model_type}", key="train_btn"):
        with st.spinner(f"Entrenando {st.session_state.selected_model_type}..."):
            X_selected_df = X_original[st.session_state.features_used_in_model].copy()

            # Identificar tipos de columnas para el ColumnTransformer
            numeric_features = X_selected_df.select_dtypes(include=np.number).columns.tolist()
            categorical_features = X_selected_df.select_dtypes(include=['object', 'category']).columns.tolist()

            # Crear preprocesador
            preprocessor = ColumnTransformer(
                transformers=[
                    ('num', MinMaxScaler(), numeric_features), # Escalar num√©ricas
                    ('cat', OneHotEncoder(handle_unknown='ignore', drop='first'), categorical_features) # OHE para categ√≥ricas
                ], 
                remainder='passthrough' # Dejar otras columnas (si las hubiera, aunque no deber√≠a si seleccionamos bien)
            )

            # Definir el modelo
            model_params = {'random_state': 42} # Par√°metros comunes
            if st.session_state.problem_type == "Regression":
                if st.session_state.selected_model_type == "Random Forest":
                    model = RandomForestRegressor(**model_params) # A√±adir n_estimators, max_depth si se configuraron
                elif st.session_state.selected_model_type == "Decision Tree":
                    model = DecisionTreeRegressor(**model_params)
                elif st.session_state.selected_model_type == "XGBoost":
                    model = XGBRegressor(**model_params, objective='reg:squarederror') # objective para regresi√≥n
            else: # Classification
                if st.session_state.selected_model_type == "Random Forest":
                    model = RandomForestClassifier(**model_params)
                elif st.session_state.selected_model_type == "Decision Tree":
                    model = DecisionTreeClassifier(**model_params)
                elif st.session_state.selected_model_type == "XGBoost":
                    model = XGBClassifier(**model_params, use_label_encoder=False, eval_metric='logloss' if y.nunique()==2 else 'mlogloss')

            # Crear y entrenar el Pipeline
            st.session_state.trained_pipeline = Pipeline(steps=[
                ('preprocessor', preprocessor),
                ('model', model)
            ])

            X_train, X_test, y_train, y_test = train_test_split(X_selected_df, y, test_size=0.25, random_state=42, stratify=y if st.session_state.problem_type == "Classification" and y.nunique() > 1 else None)
            
            try:
                st.session_state.trained_pipeline.fit(X_train, y_train)
                y_pred_test = st.session_state.trained_pipeline.predict(X_test)
                
                metrics = {}
                if st.session_state.problem_type == "Regression":
                    metrics['R-squared'] = r2_score(y_test, y_pred_test)
                    metrics['MSE'] = mean_squared_error(y_test, y_pred_test)
                    metrics['RMSE'] = np.sqrt(metrics['MSE'])
                else: # Classification
                    # Si y_test fue codificada y el pipeline predice la clase codificada:
                    metrics['Accuracy'] = accuracy_score(y_test, y_pred_test)
                    metrics['Precision'] = precision_score(y_test, y_pred_test, average='weighted', zero_division=0)
                    metrics['Recall'] = recall_score(y_test, y_pred_test, average='weighted', zero_division=0)
                    metrics['F1-Score'] = f1_score(y_test, y_pred_test, average='weighted', zero_division=0)
                    # Matriz de confusi√≥n (necesita clases decodificadas para etiquetas)
                    # cm = confusion_matrix(y_test, y_pred_test) # y_test son las clases codificadas
                    # if st.session_state.label_encoder_target:
                    #     cm_labels = st.session_state.label_encoder_target.classes_
                    #     # Graficar CM
                
                st.session_state.model_performance_metrics = metrics
                st.success(f"Modelo '{st.session_state.selected_model_type}' entrenado y evaluado exitosamente!")
                st.balloons()

            except Exception as e:
                st.error(f"Error durante el entrenamiento o evaluaci√≥n: {e}")
                st.session_state.trained_pipeline = None
                st.session_state.model_performance_metrics = None
    
    # Mostrar m√©tricas si el modelo est√° entrenado
    if st.session_state.trained_pipeline and st.session_state.model_performance_metrics:
        st.subheader("M√©tricas de Desempe√±o del Modelo (en conjunto de prueba):")
        for metric_name, metric_value in st.session_state.model_performance_metrics.items():
            st.write(f"- **{metric_name}:** {metric_value:.4f}")
        
        # (Opcional) Mostrar matriz de confusi√≥n para clasificaci√≥n
        if st.session_state.problem_type == "Classification":
            y_pred_test_for_cm = st.session_state.trained_pipeline.predict(X_test) # X_test ya est√° definida arriba
            # y_test_for_cm = y_test # y_test ya est√° definida arriba y es la codificada
            
            # Decodificar etiquetas para la matriz de confusi√≥n si es posible
            y_test_decoded_cm = st.session_state.label_encoder_target.inverse_transform(y_test) if st.session_state.label_encoder_target else y_test
            y_pred_decoded_cm = st.session_state.label_encoder_target.inverse_transform(y_pred_test_for_cm) if st.session_state.label_encoder_target else y_pred_test_for_cm
            
            cm_labels_display = st.session_state.label_encoder_target.classes_ if st.session_state.label_encoder_target else np.unique(np.concatenate((y_test_decoded_cm, y_pred_decoded_cm)))

            cm = confusion_matrix(y_test_decoded_cm, y_pred_decoded_cm, labels=cm_labels_display)
            fig_cm, ax_cm = plt.subplots()
            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=cm_labels_display, yticklabels=cm_labels_display, ax=ax_cm)
            ax_cm.set_xlabel('Predicho')
            ax_cm.set_ylabel('Verdadero')
            ax_cm.set_title('Matriz de Confusi√≥n')
            st.pyplot(fig_cm)
            plt.clf()
        st.markdown("---")

    # --- PASO 4: REALIZAR NUEVAS PREDICCIONES ---
    if st.session_state.trained_pipeline:
        st.header("4. Realizar Nuevas Predicciones")
        st.markdown("Ingresa los valores para las caracter√≠sticas con las que se entren√≥ el modelo:")

        # Usar las caracter√≠sticas originales que el usuario seleccion√≥
        features_for_input = st.session_state.features_used_in_model 
        
        with st.form("new_prediction_form_custom_model"):
            input_data_dict = {}
            form_cols = st.columns(2) # Para organizar el formulario
            
            for i, feature_name in enumerate(features_for_input):
                with form_cols[i % 2]:
                    original_col_series = X_original[feature_name] # Tomar de X_original para rangos y tipo
                    if pd.api.types.is_numeric_dtype(original_col_series):
                        min_val = float(original_col_series.min())
                        max_val = float(original_col_series.max())
                        mean_val = float(original_col_series.mean())
                        # Asegurar que min_val <= mean_val <= max_val
                        if not (min_val <= mean_val <= max_val): mean_val = min_val 
                        input_data_dict[feature_name] = st.number_input(
                            f"{feature_name}", 
                            min_value=min_val, 
                            max_value=max_val, 
                            value=mean_val, 
                            key=f"pred_input_{feature_name}"
                        )
                    elif pd.api.types.is_categorical_dtype(original_col_series) or original_col_series.dtype == 'object':
                        unique_values = original_col_series.unique().tolist()
                        input_data_dict[feature_name] = st.selectbox(
                            f"{feature_name}", 
                            options=unique_values, 
                            index=0, 
                            key=f"pred_input_{feature_name}"
                        )
                    else: # Fallback para tipos raros
                        input_data_dict[feature_name] = st.text_input(f"{feature_name}", key=f"pred_input_{feature_name}")
            
            submit_pred_btn = st.form_submit_button("‚ú® Obtener Predicci√≥n")

        if submit_pred_btn:
            try:
                # Crear DataFrame con los datos de entrada en el orden correcto
                input_df = pd.DataFrame([input_data_dict], columns=features_for_input)
                
                # El pipeline se encargar√° del preprocesamiento (OHE, escalado)
                prediction_coded = st.session_state.trained_pipeline.predict(input_df)
                prediction_proba = None
                if hasattr(st.session_state.trained_pipeline, "predict_proba"):
                    prediction_proba = st.session_state.trained_pipeline.predict_proba(input_df)

                # Decodificar si es clasificaci√≥n y tenemos el LabelEncoder
                final_prediction_display = prediction_coded[0]
                if st.session_state.problem_type == "Classification" and st.session_state.label_encoder_target:
                    final_prediction_display = st.session_state.label_encoder_target.inverse_transform(prediction_coded)[0]

                st.success(f"Resultado de la Predicci√≥n: **{final_prediction_display}**")

                if prediction_proba is not None and st.session_state.problem_type == "Classification":
                    st.write("Probabilidades por clase:")
                    classes_ = st.session_state.label_encoder_target.classes_ if st.session_state.label_encoder_target else \
                               st.session_state.trained_pipeline.named_steps['model'].classes_ # Fallback
                    proba_df = pd.DataFrame(prediction_proba, columns=classes_)
                    st.dataframe(proba_df)

            except Exception as e:
                st.error(f"Error al realizar la predicci√≥n: {e}")
                st.error("Aseg√∫rate de que los datos de entrada sean v√°lidos y compatibles.")
